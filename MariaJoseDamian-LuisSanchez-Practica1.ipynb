{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<center> <h1>APRENDIZAJE POR REFUERZO</h1> </center>\n",
        "<center> <h2>Práctica 1: SARSA y Q-LEARNING</h2> </center>\n",
        "<center> <h4>Luis Sánchez Polo</h4> </center>\n",
        "<center> <h4>María José Damián Diez</h4> </center>\n",
        "<center> <h5>7 de mayo de 2023</h5> </center>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nYiOkZrdM4GE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3PbFgd6CpS1"
      },
      "source": [
        "# GridWorld 2:\n",
        "\n",
        "*GridWorld* es un mundo en forma de cuadrícula muy utilizado como entorno de pruebas para técnicas de Aprendizaje por Refuerzo. Dentro de esta cuadrícula hay varios tipos de celdas: iniciales, libres, obstáculos, terminales... ¡y ahora también agujeros de gusano! Los agentes tienen que llegar desde una celda inicial hasta otra terminal evitando los obtáculos y recorriendo una distancia mínima."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLz6HYMXCyID"
      },
      "source": [
        "Paquetes necesarios para *GridWorld 2*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "aet9gV2KCe2q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4fsv0HwszqX"
      },
      "source": [
        "Funciones auxiliares para visualizar información:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "awRXzniVs5pj"
      },
      "outputs": [],
      "source": [
        "def printMap(world):\n",
        "  # Visualiza el mapa de GridWorld\n",
        "  m = \"[\"\n",
        "  for i in range(world.size[0]):\n",
        "    for j in range(world.size[1]):\n",
        "      if world.map[(i, j)] == 0: \n",
        "        m += \" O \"\n",
        "      elif world.map[(i, j)] == -1:\n",
        "        m += \" X \" \n",
        "      elif world.map[(i, j)] == 1:\n",
        "        m += \" F \"\n",
        "      elif world.map[(i, j)] == 2:\n",
        "        m += \" T \"\n",
        "    if i == world.size[0] - 1:\n",
        "      m += \"]\\n\"\n",
        "    else:\n",
        "      m += \"\\n\"\n",
        "  print(m)\n",
        "\n",
        "def printPolicy(world, policy):\n",
        "  # Visualiza la política con flechas\n",
        "  p = \"\"\n",
        "  for i in range(world.size[0]):\n",
        "    for j in range(world.size[1]):\n",
        "      color = \"\\033[0m\"\n",
        "      if world.map[(i, j)] == -1:\n",
        "        p += color + \" X \"\n",
        "      elif world.map[(i, j)] == 1:\n",
        "        p += color + \" F \"\n",
        "      else:\n",
        "        if world.map[(i, j)] == 2:\n",
        "            color = \"\\033[;36;47m\"\n",
        "        if policy[i][j] == 0:\n",
        "            p += color + \" ^ \"\n",
        "        elif policy[i][j] == 1:\n",
        "            p += color + \" v \"\n",
        "        elif policy[i][j] == 2:\n",
        "            p += color + \" < \"\n",
        "        else:\n",
        "            p += color + \" > \"\n",
        "    p += \"\\n\"\n",
        "  print(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELyNLQplC-W0"
      },
      "source": [
        "# Clase *World*: \n",
        "\n",
        "Esta clase almacena la información del mundo:\n",
        "\n",
        "*   *Map*: Matriz con la codificación del mundo con celdas libres (0), obstáculos (-1) y terminales (1)\n",
        "*   *Size*: Vector con el tamaño de la matriz de codificación del mundo (ancho, alto)\n",
        "\n",
        "Para crear un mundo hay que aportar los siguientes datos:\n",
        "\n",
        "*   Tamaño del mapa (ancho, alto)\n",
        "*   Lista de celdas terminales\n",
        "*   Lista de celdas con obstáculos\n",
        "*   Agujero de gusano\n",
        "\n",
        "Notas:\n",
        "\n",
        "* Cuando el agente cae en un obstáculo se queda atrapado para siempre en él\n",
        "* Cuando el agente entra por un extremo del agujero de gusano sale por el otro extremo\n",
        "\n",
        "Por ejemplo: \n",
        "\n",
        "w = World((10, 10), [(9, 9)], [(2, 4), (4, 2)], [(0, 2), (9, 7)])\n",
        "\n",
        "Crea un mundo de 10 filas y 10 columnas con un estado terminal (9, 9), dos obstáculos en (2, 4) y (4, 2) y un teletransporte entre (0, 2) y (9, 7).\n",
        "\n",
        "![map2.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "v7iQbNj6C9MC"
      },
      "outputs": [],
      "source": [
        "class World:\n",
        "\n",
        "  def __init__(self, size, terminal, obstacle, hole):\n",
        "    # Crea un mundo\n",
        "    self.size = size\n",
        "    self.map = {}\n",
        "    for i in range(size[0]):\n",
        "      for j in range(size[1]):\n",
        "        # Estados libres\n",
        "        self.map[(i, j)] = 0\n",
        "        # Estados terminales\n",
        "        for t in terminal:\n",
        "          if i==t[0] and j==t[1]:\n",
        "            self.map[(i, j)] = 1\n",
        "        # Estados con obstáculos\n",
        "        for o in obstacle:\n",
        "          if i==o[0] and j==o[1]:\n",
        "            self.map[(i, j)] = -1\n",
        "        for h in hole:\n",
        "          if i==h[0] and j==h[1]:\n",
        "            self.map[(i, j)] = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTrYC15IHy3s"
      },
      "source": [
        "Prueba de la clase *World*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgBnrzsQHS8R",
        "outputId": "420ad006-a146-48cc-9d57-475a0767aea4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ O  O  T  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  X  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O \n",
            " O  O  X  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  T  O  F ]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  w = World((10, 10), [(9, 9)], [(2, 4), (4, 2)], [(0, 2), (9, 7)])\n",
        "  printMap(w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yw3UEuAI6Cv"
      },
      "source": [
        "# Clase *Agent*:\n",
        "\n",
        "Esta clase controla el agente que aprende por refuerzo en *GridWorld*. \n",
        "\n",
        "Para crear un agente se necesitan los siguientes datos:\n",
        "\n",
        "*   *World*: Mundo en el que se desenvuelve el agente.\n",
        "*   *Initial State*: Estado inicial del agente.\n",
        "\n",
        "Para controlar el agente se usan los siguientes métodos:\n",
        "\n",
        "*   *nextState = move(state, action)*: Mueve el agente del estado *state* a un nuevo estado *nextState* aplicando una acción *action*.\n",
        "*   *reward = reward(nextState)*: Devuelve el refuerzo *reward* que recibe el agente al transicionar al estado *nextState*.\n",
        "*   *nextState, reward = checkAction(state, action)*: Comprueba a qué estado *nextState* y con qué refuerzo *reward* cambia el agente al aplicar la acción *action* en el estado *state*. Este método no cambia el estado interno del agente, por lo que puede usarse para hacer barridos del espacio de estados.\n",
        "*   *nextState, reward = executeAction(action)*: Ejecuta la acción *action* en el estado actual y devuelve el nuevo estado *nextState* y el refuerzo *reward*. Este método cambia el estado interno del agente, por lo que sólo debe usarse cuando se realice un recorrido por el mundo.\n",
        "\n",
        "Nota: Podéis hacer cambios en el agente (distribución de refuerzos, comportamiento en obstáculos...) buscando mejorar el rendimiento de los algoritmos. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2as_2oTqH61U"
      },
      "outputs": [],
      "source": [
        "class Agent:\n",
        "\n",
        "  def __init__(self, world, initialState):\n",
        "    # Crea un agente\n",
        "    self.world = world\n",
        "    self.state = np.array(initialState)\n",
        "\n",
        "  def move(self, state, action):\n",
        "    # Gestiona las transiciones de estados\n",
        "    nextState = state + np.array(action)\n",
        "    if nextState[0] < 0:\n",
        "      nextState[0] = 0\n",
        "    elif nextState[0] >= self.world.size[0]:\n",
        "      nextState[0] = self.world.size[0] - 1\n",
        "    if nextState[1] < 0:\n",
        "      nextState[1] = 0\n",
        "    elif nextState[1] >= self.world.size[1]:\n",
        "      nextState[1] = self.world.size[1] - 1\n",
        "    if self.world.map[(nextState[0], nextState[1])] == 2:\n",
        "      aux = nextState\n",
        "      for i in range(self.world.size[0]):\n",
        "        for j in range(self.world.size[1]):\n",
        "          if self.world.map[(i, j)] == 2 and (nextState[0] != i and nextState[1] != j):\n",
        "            aux = np.array([i, j])\n",
        "            nextState = aux\n",
        "    return nextState\n",
        "\n",
        "  def reward(self, nextState):\n",
        "    # Gestiona los refuerzos\n",
        "    if self.world.map[(nextState[0], nextState[1])] == -1:\n",
        "      # Refuerzo cuando el agente intenta moverse a un obstáculo\n",
        "      reward = -1 # ** Prueba varios valores **\n",
        "    elif self.world.map[(nextState[0], nextState[1])] == 1:\n",
        "      # Refuerzo cuando el agente se mueve a una celda terminal\n",
        "      reward = 1 # ** Prueba varios valores **\n",
        "    else:\n",
        "      # Refuerzo cuando el agente se mueve a una celda libre\n",
        "      reward = 0 # ** Prueba varios valores ** \n",
        "    return reward\n",
        "\n",
        "  def checkAction(self, state, action):\n",
        "    # Planifica una acción\n",
        "    nextState = self.move(state, action)\n",
        "    if self.world.map[(state[0], state[1])] == -1: \n",
        "      nextState = state                            \n",
        "    reward = self.reward(nextState)\n",
        "    return nextState, reward\n",
        "\n",
        "  def executeAction(self, action):\n",
        "    # Planifica y ejecuta una acción\n",
        "    nextState = self.move(self.state, action)\n",
        "    if self.world.map[(self.state[0], self.state[1])] == -1: \n",
        "      nextState = self.state     \n",
        "    else: \n",
        "      self.state = nextState                                 \n",
        "    reward = self.reward(nextState)\n",
        "    return self.state, reward  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIpqVYBwMid7"
      },
      "source": [
        "Prueba de la clase *Agent*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UG2bfPrfJFg4",
        "outputId": "c158dfe2-c7eb-453b-d77f-ef77707fc0b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ O  O  T  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  X  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O \n",
            " O  O  X  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  T  O  F ]\n",
            "\n",
            "(array([0, 1]), 0)\n",
            "(array([9, 7]), 0)\n",
            "(array([9, 8]), 0)\n",
            "(array([9, 9]), 1)\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  # Crear el mundo\n",
        "  w = World((10, 10), [(9, 9)], [(2, 4), (4, 2)], [(0, 2), (9, 7)])\n",
        "  printMap(w)\n",
        "  # Crear el agente\n",
        "  a = Agent(w, (0, 0))\n",
        "  # Mover el agente en la diagonal principal\n",
        "  for i in range(1, 5):\n",
        "    # Mostrar cada nuevo estado y su recompensa\n",
        "    print(a.executeAction((0, 1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9whMl4OxJI-"
      },
      "source": [
        "# Trabajo:\n",
        "\n",
        "En este trabajo vais a implementar los dos algoritmos más comunes de aprendizaje por refuerzo basados en el valor: SARSA y QLearning. Además, vais a probar ambos algoritmos en una serie de escenarios para evaluar su funcionamiento y comparar sus resultados.\n",
        "\n",
        "## Mundos: \n",
        "\n",
        "Para probar los algoritmos se ofrecen los siguientes mundos en varios tamaños:\n",
        "\n",
        "*   Mundo 1: Laberinto fácil que se puede recorrer en zigzag\n",
        "*   Mundo 2: Mundo con obstáculos aleatorios en el que el teletransporte acorta la distancia desde el inicio hasta el final\n",
        "*   Mundo 3: Mundo con obstáculos aleatorios en el que el teletransporte no reduce la distancia desde el inicio hasta el final\n",
        "*   Mundo 4: Laberinto difícil con caminos correctos y equivocados\n",
        "\n",
        "Nota: Sentíos libres de utilizar todos o algunos de estos escenarios o directamente crear vuestros propios escenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f9kCBErMweC",
        "outputId": "641b17fe-f72c-4d91-f7ea-613987c857f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "World 1: \n",
            "[ O  X  O  O  O \n",
            " O  X  O  X  O \n",
            " O  X  O  X  O \n",
            " O  X  O  X  O \n",
            " O  O  O  X  F ]\n",
            "\n",
            "World 1: \n",
            "[ O  X  O  O  O  X  O  O  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  O  O  X  O  O  O  X  F ]\n",
            "\n",
            "World 1: \n",
            "[ O  X  O  O  O  X  O  O  O  X  O  O  O  X  O  O  O  X  O  O  O \n",
            " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O  X  O \n",
            " O  O  O  X  O  O  O  X  O  O  O  X  O  O  O  X  O  O  O  X  F ]\n",
            "\n",
            "World 2: \n",
            "[ O  O  O  O  O \n",
            " O  O  O  O  O \n",
            " T  O  O  X  O \n",
            " O  O  X  X  O \n",
            " O  O  T  O  F ]\n",
            "\n",
            "World 2: \n",
            "[ O  O  O  O  O  O  O  O  O  O \n",
            " O  X  X  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O \n",
            " O  T  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  X  O  X  O  O \n",
            " O  O  X  O  O  O  O  O  O  O \n",
            " O  O  O  X  O  O  O  O  O  O \n",
            " O  X  O  X  O  O  O  O  O  O \n",
            " O  O  O  X  O  O  T  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  F ]\n",
            "\n",
            "World 2: \n",
            "[ O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  X  O  O  X  O  O  O  O  O  O  X  O  O  O  O  O \n",
            " O  O  O  O  X  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O  O  O  O  O  X  O  O  X  O  O  O \n",
            " O  O  O  O  O  O  O  X  O  O  X  O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
            " O  X  T  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
            " O  X  O  O  O  O  O  X  O  X  O  O  O  O  X  X  O  X  X  O  O \n",
            " O  O  X  X  O  O  O  O  O  O  O  O  O  X  X  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O  O  O  O  X  O  O  X  O  O  O  O \n",
            " O  O  X  X  O  O  O  X  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O  O  O  O  O  X  O  O  X  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O  O  O  O  O  X  O  X  X  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O  O  O  O  O  X  O  O  O  X  O  O \n",
            " O  O  O  O  O  O  O  O  O  O  O  X  O  O  O  O  O  O  X  O  O \n",
            " O  O  O  O  O  O  O  O  O  O  O  O  O  X  O  O  X  O  X  O  O \n",
            " O  O  O  O  O  O  X  O  X  O  O  O  X  X  O  O  X  O  X  O  O \n",
            " O  X  O  O  O  X  O  O  O  O  O  O  O  O  T  O  X  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  F ]\n",
            "\n",
            "World 3: \n",
            "[ O  O  O  O  T \n",
            " O  X  O  O  O \n",
            " O  X  X  O  O \n",
            " O  O  O  O  O \n",
            " T  O  O  O  F ]\n",
            "\n",
            "World 3: \n",
            "[ O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  T  O \n",
            " O  O  O  X  O  O  O  X  O  O \n",
            " O  O  X  O  X  O  O  O  O  O \n",
            " O  O  O  X  O  X  O  O  O  O \n",
            " O  O  O  O  O  X  O  X  O  O \n",
            " O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  X  O  O  O  O  O  O \n",
            " O  T  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  F ]\n",
            "\n",
            "World 3: \n",
            "[ O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
            " O  X  O  O  O  O  X  O  O  O  O  O  X  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  X  O  O  T  O  O \n",
            " O  X  O  O  O  O  O  X  O  O  O  O  O  X  O  X  O  O  X  O  O \n",
            " O  O  O  O  O  O  X  X  O  O  O  O  O  X  X  O  O  X  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  X  X  O  O  O  O  X  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  X  O  O  O  X  O  O  O  O  X  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
            " O  X  O  O  O  O  O  O  O  O  O  X  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
            " O  O  X  X  O  O  O  O  O  O  O  O  O  O  X  O  O  X  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O  O  X  O  O  O  O  O  O  X  O  O \n",
            " O  O  O  X  O  O  X  O  X  O  X  O  O  O  O  X  O  X  O  O  O \n",
            " O  O  O  O  O  X  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
            " O  X  O  O  O  O  X  O  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  X  O  O  O  O \n",
            " O  O  O  O  O  X  O  X  O  O  O  O  O  X  O  X  O  X  X  O  O \n",
            " O  X  T  O  O  X  O  O  O  O  X  X  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  O  F ]\n",
            "\n",
            "World 4: \n",
            "[ O  X  O  X  O  O  O  O  O  X  O  O  O  O  O  X  X  X  O  X  O \n",
            " O  X  O  X  X  X  X  X  O  X  X  X  X  X  O  O  O  X  O  X  O \n",
            " O  X  O  O  O  O  O  O  O  X  O  O  O  X  O  X  X  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O  X  O  X  O  O  O  O  X  O  O  X  O \n",
            " O  O  O  X  O  X  O  X  X  X  X  X  X  X  X  O  X  O  X  X  O \n",
            " X  X  X  X  O  X  O  O  O  X  O  O  O  O  O  O  X  O  O  O  O \n",
            " O  O  O  O  O  X  X  X  O  X  X  X  X  X  X  O  X  X  O  X  O \n",
            " X  X  X  X  O  X  O  X  O  X  O  O  O  O  O  O  O  O  O  X  O \n",
            " O  O  O  X  O  O  O  X  X  X  O  O  X  X  X  X  X  X  X  X  O \n",
            " O  X  O  X  O  X  O  X  O  O  O  X  X  O  O  O  O  O  O  X  X \n",
            " O  X  O  X  O  X  X  X  O  X  O  X  O  O  X  X  X  X  O  O  O \n",
            " O  X  O  X  O  X  O  O  O  X  O  X  O  X  X  O  O  X  X  X  O \n",
            " O  X  O  O  O  X  X  O  X  X  O  X  O  X  O  O  O  O  O  X  O \n",
            " O  X  X  X  X  X  O  O  X  O  O  O  O  O  O  X  X  X  O  X  O \n",
            " O  O  O  O  X  O  O  X  X  O  X  O  X  X  O  X  O  O  O  X  O \n",
            " X  X  X  O  O  O  X  X  O  O  X  O  O  X  X  X  O  X  X  X  X \n",
            " O  O  X  X  O  X  X  X  X  X  X  X  O  O  O  X  O  X  O  O  O \n",
            " X  O  O  X  O  X  O  O  O  X  O  O  O  X  X  X  O  X  O  X  O \n",
            " X  X  O  O  O  X  X  X  O  X  X  X  O  O  O  X  O  O  O  X  O \n",
            " O  X  X  O  X  X  O  O  O  O  O  X  O  X  X  X  X  X  X  X  O \n",
            " O  O  O  O  O  O  O  X  X  X  O  X  O  O  O  O  O  O  O  X  F ]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  \n",
        "  # Mundo 1 pequeño: Laberinto fácil\n",
        "  obstacles = []\n",
        "  for j in range(0, 4):\n",
        "    obstacles.append((j, 1))\n",
        "  for j in range(1, 5):\n",
        "    obstacles.append((j, 3))\n",
        "  w1p = World((5, 5), [(4, 4)], obstacles, [])\n",
        "  print(\"World 1: \")\n",
        "  printMap(w1p)\n",
        "\n",
        "  # Mundo 1 mediano: Laberinto fácil\n",
        "  obstacles = []\n",
        "  for i in [1, 5]:\n",
        "    for j in range(0, 8):\n",
        "      obstacles.append((j, i))\n",
        "  for i in [3, 7]:\n",
        "    for j in range(1, 9):\n",
        "      obstacles.append((j, i))\n",
        "  w1m = World((9, 9), [(8, 8)], obstacles, [])\n",
        "  print(\"World 1: \")\n",
        "  printMap(w1m)\n",
        "\n",
        "  # Mundo 1 grande: Laberinto fácil\n",
        "  obstacles = []\n",
        "  for i in [1, 5, 9, 13, 17]:\n",
        "    for j in range(0, 20):\n",
        "      obstacles.append((j, i))\n",
        "  for i in [3, 7, 11, 15, 19]:\n",
        "    for j in range(1, 21):\n",
        "      obstacles.append((j, i))\n",
        "  w1g = World((21, 21), [(20, 20)], obstacles, [])\n",
        "  print(\"World 1: \")\n",
        "  printMap(w1g)\n",
        "\n",
        "  # Mundo 2 pequeño: Obstáculos aleatorios, teletransporte útil\n",
        "  obstacles = []\n",
        "  for i in range(3):\n",
        "    obstacles.append((np.random.randint(1, 4), np.random.randint(1, 4)))  \n",
        "  w2p = World((5, 5), [(4, 4)], obstacles, [(2, 0), (4, 2)])\n",
        "  print(\"World 2: \")\n",
        "  printMap(w2p)\n",
        "\n",
        "  # Mundo 2 mediano: Obstáculos aleatorios, teletransporte útil\n",
        "  obstacles = []\n",
        "  for i in range(10):\n",
        "    obstacles.append((np.random.randint(1, 9), np.random.randint(1, 9)))  \n",
        "  w2m = World((10, 10), [(9, 9)], obstacles, [(3, 1), (8, 6)])\n",
        "  print(\"World 2: \")\n",
        "  printMap(w2m)\n",
        "\n",
        "  # Mundo 2 grande: Obstáculos aleatorios, teletransporte útil\n",
        "  obstacles = []\n",
        "  for i in range(50):\n",
        "    obstacles.append((np.random.randint(1, 19), np.random.randint(1, 19)))  \n",
        "  w2g = World((21, 21), [(20, 20)], obstacles, [(6, 2), (18, 14)])\n",
        "  print(\"World 2: \")\n",
        "  printMap(w2g)\n",
        "\n",
        "  # Mundo 3 pequeño: Obstáculos aleatorios, teletransporte inútil\n",
        "  obstacles = []\n",
        "  for i in range(3):\n",
        "    obstacles.append((np.random.randint(1, 4), np.random.randint(1, 4)))  \n",
        "  w3p = World((5, 5), [(4, 4)], obstacles, [(4, 0), (0, 4)])\n",
        "  print(\"World 3: \")\n",
        "  printMap(w3p)\n",
        "\n",
        "  # Mundo 3 mediano: Obstáculos aleatorios, teletransporte inútil\n",
        "  obstacles = []\n",
        "  for i in range(10):\n",
        "    obstacles.append((np.random.randint(1, 9), np.random.randint(1, 9)))  \n",
        "  w3m = World((10, 10), [(9, 9)], obstacles, [(8, 1), (1, 8)])\n",
        "  print(\"World 3: \")\n",
        "  printMap(w3m)\n",
        "\n",
        "  # Mundo 3 grande: Obstáculos aleatorios, teletransporte inútil\n",
        "  obstacles = []\n",
        "  for i in range(50):\n",
        "    obstacles.append((np.random.randint(1, 19), np.random.randint(1, 19)))  \n",
        "  w3g = World((21, 21), [(20, 20)], obstacles, [(18, 2), (2, 18)])\n",
        "  print(\"World 3: \")\n",
        "  printMap(w3g)\n",
        "\n",
        "  # Mundo 4: Laberinto difícil\n",
        "  obstacles = [(0,1),(0,3),(0,9),(0,15),(0,16),(0,17),(0,19),\n",
        "               (1,1),(1,3),(1,4),(1,5),(1,6),(1,7),(1,9),(1,10),(1,11),(1,12),(1,13),(1,17),(1,19),\n",
        "               (2,1),(2,9),(2,13),(2,15),(2,16),(2,17),(2,19),\n",
        "               (3,1),(3,3),(3,5),(3,7),(3,9),(3,11),(3,16),(3,19),\n",
        "               (4,3),(4,5),(4,7),(4,8),(4,9),(4,10),(4,11),(4,12),(4,13),(4,14),(4,16),(4,18),(4,19),\n",
        "               (5,0),(5,1),(5,2),(5,3),(5,5),(5,9),(5,16),\n",
        "               (6,5),(6,6),(6,7),(6,9),(6,10),(6,11),(6,12),(6,13),(6,14),(6,16),(6,17),(6,19),\n",
        "               (7,0),(7,1),(7,2),(7,3),(7,5),(7,7),(7,9),(7,19),\n",
        "               (8,3),(8,7),(8,8),(8,9),(8,12),(8,13),(8,14),(8,15),(8,16),(8,17),(8,18),(8,19),\n",
        "               (9,1),(9,3),(9,5),(9,7),(9,11),(9,12),(9,19),(9,20),\n",
        "               (10,1),(10,3),(10,5),(10,6),(10,7),(10,9),(10,11),(10,14),(10,15),(10,16),(10,17),\n",
        "               (11,1),(11,3),(11,5),(11,9),(11,11),(11,13),(11,14),(11,17),(11,18),(11,19),\n",
        "               (12,1),(12,5),(12,6),(12,8),(12,9),(12,11),(12,13),(12,19),\n",
        "               (13,1),(13,2),(13,3),(13,4),(13,5),(13,8),(13,15),(13,16),(13,17),(13,19),\n",
        "               (14,4),(14,7),(14,8),(14,10),(14,12),(14,13),(14,15),(14,19),\n",
        "               (15,0),(15,1),(15,2),(15,6),(15,7),(15,10),(15,13),(15,14),(15,15),(15,17),(15,18),(15,19),(15,20),\n",
        "               (16,2),(16,3),(16,5),(16,6),(16,7),(16,8),(16,9),(16,10),(16,11),(16,15),(16,17),\n",
        "               (17,0),(17,3),(17,5),(17,9),(17,13),(17,14),(17,15),(17,17),(17,19),\n",
        "               (18,0),(18,1),(18,5),(18,6),(18,7),(18,9),(18,10),(18,11),(18,15),(18,19),\n",
        "               (19,1),(19,2),(19,4),(19,5),(19,11),(19,13),(19,14),(19,15),(19,16),(19,17),(19,18),(19,19),\n",
        "               (20,7),(20,8),(20,9),(20,11),(20,19)]          \n",
        "  print(\"World 4: \")\n",
        "  w4 = World((21, 21), [(20, 20)], obstacles, [])\n",
        "  printMap(w4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBuT8CQk17QE"
      },
      "source": [
        "## SARSA:\n",
        "\n",
        "*SARSA* (State-Action-Reward-State-Action) es un método basado en el valor que permite resolver problemas de aprendizaje por refuerzo. Al igual que el resto de métodos basados en el valor, *SARSA* calcula de forma iterativa la función de valor $Q(S,A)$ y, a partir de ella, determina la política óptima $\\pi$.\n",
        "\n",
        "*SARSA* recibe su nombre de las cinco variables implicadas en su función de actualización: el estado actual ($S_t$), la acción actual ($A_t$), el refuerzo actual ($R_t$), el siguiente estado ($S_{t+1}$) y la siguiente acción ($A_{t+1}$). Esta función de actualización tiene la siguiente forma:\n",
        "\n",
        "\\begin{equation}\n",
        "Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha [R_t + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t,A_t)]\n",
        "\\end{equation}\n",
        "\n",
        "Nota: $\\alpha$ es la longitud del episodio y $\\gamma$ el factor de descuento.\n",
        "\n",
        "El algoritmo *SARSA* sigue el siguiente esquema: \n",
        "\n",
        "1.   Inicializar $Q(S,A)$ para todos los estados y acciones \n",
        "2.   **Bucle** (repetir $3-9$ hasta la convergencia):\n",
        "3.   Inicializar $S_t$\n",
        "4.   Elegir $A_t$ en $S_t$ siguiendo la política derivada de $Q(S,A)$\n",
        "5.   **Bucle** (repetir $6-9$ hasta que $S_t$ sea terminal):\n",
        "6.   Tomar la acción $A_t$ en $S_t$ y observar $R_t$ y $S_{t+1}$\n",
        "7.   Elegir $A_{t+1}$ en $S_{t+1}$ siguiendo la política derivada de $Q(S,A)$\n",
        "8.   Actualizar el valor $Q(S_t, A_t)$ con la función de actualización\n",
        "9.   Tomar $S_{t+1}$ y $A_{t+1}$ como los nuevos $S_t$ y $A_t$\n",
        "\n",
        "El algoritmo *SARSA* utiliza un parámetro $\\epsilon \\in (0, 1)$ para buscar un equilibrio entre exploración y explotación. A la hora de elegir $A_t$ en $S_t$, si un número aleatorio es menor que $\\epsilon$, el algoritmo tomará una acción aleatoria; mientras que si ese número aleatorio es mayor que $\\epsilon$, el algoritmo tomará la mejor acción conocida.\n",
        "\n",
        "## Ejercicio 1:\n",
        "\n",
        "Implementad el algoritmo SARSA para el agente y entorno definidos anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QKt3TQpqWpjX"
      },
      "outputs": [],
      "source": [
        "# Funciones auxiliares:\n",
        "\n",
        "def choose_action(Q, S, actions, epsilon):\n",
        "  ''' Escoge una acción, ya sea por exploración o explotación para los estados '''\n",
        "\n",
        "  if np.random.rand() < epsilon:\n",
        "    action = np.random.choice(len(actions)) # Acción por exploración\n",
        "  else:\n",
        "    action = np.argmax(Q[S[0], S[1], :]) # Acción por explotación\n",
        "  return action \n",
        "\n",
        "\n",
        "def random_state(world):\n",
        "  '''Escoge la posición inicial del estado de forma aleatoria considerando el tamaño del mundo'''\n",
        "\n",
        "  # Obtener el tamaño del mundo\n",
        "  size = world.size\n",
        "  \n",
        "  # Elegir una posición aleatoria que no sea un obstáculo\n",
        "  while True:\n",
        "    x = random.randint(0, size[0]-1)\n",
        "    y = random.randint(0, size[1]-1)\n",
        "    if world.map[(x, y)] not in [-1, 1]:\n",
        "        break\n",
        "  \n",
        "  # Devolver la posición como estado\n",
        "  state = [x,y]\n",
        "  return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ZmAqon5N62HB"
      },
      "outputs": [],
      "source": [
        "# Resolución: Código de SARSA\n",
        "\n",
        "def SARSA(world, actions, S, alpha=0.5, gamma=0.9, epsilon=0.1, max_iterations=10000, contador_max=100):\n",
        "  \n",
        "  # Crear agente\n",
        "  agent = Agent(world, S)\n",
        "\n",
        "  # 1. Inicializar Q\n",
        "  num_actions = len(actions)\n",
        "  Q = np.zeros((world.size[0], world.size[1], num_actions))\n",
        "\n",
        "  # 2. Bucle hasta que converja o cumpla el máximo de iteraciones \n",
        "  episode = 0\n",
        "  while episode < max_iterations:\n",
        "    # 3. Inicializar S\n",
        "    if episode < max_iterations/4:\n",
        "      state = random_state(world)\n",
        "    else:\n",
        "      state = S\n",
        "    \n",
        "    # 4. Elegir A en S siguiendo la política derivada de Q(S,A)    \n",
        "    reward_sum = 0\n",
        "    contador = 0\n",
        "\n",
        "    # 5. Bucle hasta S sea terminal\n",
        "    while contador < contador_max:\n",
        "\n",
        "      idx_action = choose_action(Q, S, actions, epsilon)\n",
        "      action = actions[idx_action]\n",
        "\n",
        "      # 6. Tomar la acción en S y observar reward next_S\n",
        "      next_state, reward = agent.checkAction(state, action) #cambiar\n",
        "      #print(state, next_state, reward)\n",
        "\n",
        "      #7. Elegir nueva acción\n",
        "      next_idx_action = choose_action(Q, next_state, actions, epsilon)\n",
        "      \n",
        "      # 8. Actualizar Q\n",
        "      Q[state[0], state[1], idx_action] += alpha * (reward + gamma * Q[next_state[0], next_state[1], next_idx_action] - Q[state[0], state[1], idx_action])\n",
        "\n",
        "      # 9. Actualizar estado y acción\n",
        "      state = next_state\n",
        "      action = actions[next_idx_action]\n",
        "      reward_sum += reward\n",
        "\n",
        "      # Comprobar si se ha llegado a un estado terminal o a un obstáculo\n",
        "      if world.map[(state[0], state[1])] in [-1, 1]:\n",
        "        break\n",
        "      \n",
        "      contador +=1\n",
        "          \n",
        "      #Siguiente episodio\n",
        "    episode += 1\n",
        "\n",
        "  # Política aprendida\n",
        "  policy = np.argmax(Q, axis=2)\n",
        "  print('Mundo')\n",
        "  printMap(world)\n",
        "  print('Política')\n",
        "  printPolicy(world,policy)\n",
        "  print(policy)\n",
        "  \n",
        "  return print(\"\\nEpisodios: {}\".format(episode))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-aKUuoX_Ut-"
      },
      "source": [
        "## Q-Learning:\n",
        "\n",
        "*Q-Learning* es el método más conocido para resolver problemas de aprendizaje por refuerzo mediante un esquema basado en el valor. Este algoritmo recibe su nombre directamente de $Q(S,A)$, la función de valor que va actualizando a lo largo de su ejecución. *Q-Learning* es muy parecido a *SARSA*, pero tiene una función de actualización diferente:\n",
        "\n",
        "\\begin{equation}\n",
        "Q(S_t,A_t) \\leftarrow Q(S_t,A_t) + \\alpha [R_t + \\gamma max_a{Q(S_{t+1}}, a) - Q(S_t,A_t)]\n",
        "\\end{equation}\n",
        "\n",
        "En este caso, la acción $A_{t+1}$ en $S_{t+1}$ se toma buscando el máximo valor, en lugar de poder elegir entre exploración o explotación.\n",
        "\n",
        "El algoritmo *Q-Learning* sigue el siguiente esquema: \n",
        "\n",
        "1.   Inicializar $Q(S,A)$ para todos los estados y acciones \n",
        "2.   **Bucle** (repetir $3-8$ hasta la convergencia):\n",
        "3.   Inicializar $S_t$\n",
        "4.   **Bucle** (repetir $6-8$ hasta que $S_t$ sea terminal):\n",
        "5.   Elegir $A_t$ en $S_t$ siguiendo la política derivada de $Q(S,A)$\n",
        "6.   Tomar la acción $A_t$ en $S_t$ y observar $R_t$ y $S_{t+1}$\n",
        "7.   Actualizar el valor $Q(S_t, A_t)$ con la función de actualización\n",
        "8.   Tomar $S_{t+1}$ como el nuevo $S_t$\n",
        "\n",
        "## Ejercicio 2:\n",
        "Implementad el algoritmo Q-Learning para el agente y entorno definidos anteriormente.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "e2sahj_2Oj4k"
      },
      "outputs": [],
      "source": [
        "# Resolución: Código de Q-Learning\n",
        "def Q_Learning(world, actions, S, alpha=0.5, gamma=1, epsilon=0.1, max_iterations=1000, contador_max=100):\n",
        "    \n",
        "  # Crear agente\n",
        "  agent = Agent(world, S)\n",
        "\n",
        "  # 1. Inicializar Q\n",
        "  num_actions = len(actions)\n",
        "  Q = np.zeros((world.size[0], world.size[1], num_actions))\n",
        "\n",
        "  # 2. Bucle hasta que converja o cumpla el máximo de iteraciones \n",
        "  episode = 0\n",
        "  while episode < max_iterations:\n",
        "    # 3. Inicializar S\n",
        "    if episode < max_iterations/3:\n",
        "      state = random_state(world)\n",
        "    else:\n",
        "      state = S\n",
        "    reward_sum = 0\n",
        "    contador = 0\n",
        "    # 4. Bucle hasta que S sea terminal\n",
        "    while contador < contador_max:\n",
        "      # 5. Elegir A en S siguiendo la política derivada de Q(S,A) \n",
        "      idx_action = choose_action(Q, S, actions, epsilon)\n",
        "      action = actions[idx_action]\n",
        "      \n",
        "      # 6.Tomar la acción en S y observar reward next_S\n",
        "      next_state, reward = agent.checkAction(state, action)\n",
        "\n",
        "      # 7. Actualizar Q\n",
        "      Q[state[0], state[1], idx_action] += alpha * (reward + gamma * np.max(Q[next_state[0], next_state[1], :]) - Q[state[0], state[1], idx_action])\n",
        "\n",
        "      # 8. Actualizar estado \n",
        "      state = next_state\n",
        "      reward_sum += reward\n",
        "\n",
        "      # Comprobar si se ha llegado a un estado terminal o a un obstáculo\n",
        "      if world.map[(state[0], state[1])] in [1, -1]:\n",
        "          break\n",
        "\n",
        "      contador +=1 \n",
        "      \n",
        "    #Siguiente episodio\n",
        "    episode += 1\n",
        "\n",
        "  # Política aprendida\n",
        "  policy = np.argmax(Q, axis=2)\n",
        "\n",
        "  print('Mundo')\n",
        "  printMap(world)\n",
        "  print('Política')\n",
        "  printPolicy(world,policy)\n",
        "  print(policy)\n",
        "  \n",
        "  return print(\"\\nEpisodios: {}\".format(episode))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD_2_U4PDvPG"
      },
      "source": [
        "## Análisis:\n",
        "\n",
        "*SARSA* y *Q-Learning* son dos algoritmos muy parecidos, que se pueden aplicar en los mismos problemas y suelen encontrar soluciones similares. No obstante, los resultados de ambos algoritmos pueden diferir en ciertos problemas: por ejemplo, hay un problema llamado Cliffworld en el que SARSA encuentra soluciones más seguras y con menos valor, mientras que Q-Learning asume más riesgos y consigue más valor ([artículo interesante](https://medium.com/gradientcrescent/fundamentals-of-reinforcement-learning-navigating-cliffworld-with-sarsa-and-q-learning-cc3c36eb5830)).\n",
        "\n",
        "## Ejercicio 3:\n",
        "\n",
        "Analizad los resultados obtenidos por ambos algoritmos en los escenarios de prueba.\n",
        "\n",
        "1.   Comentad el rendimiento que observáis en ambos algoritmos. ¿Qué problemas son capaces de resolver? ¿En cuáles no encuentran la solución óptima? ¿A qué se puede deber este comportamiento?\n",
        "\n",
        "2.   Comentad las diferencias entre los algoritmos en los diferentes escenarios: ¿Cuál resuelve más escenarios? ¿Cuál converge más rápido? ¿Cuál genera más valor?\n",
        "\n",
        "Nota: Las siguientes variables pueden ser interesantes para valorar los resultados: Diferencia entre la política resultante y la política óptima, número de iteraciones necesarias para converger, retorno total del problema y retorno obtenido en cada episodio.\n",
        "\n",
        "3.   Comentad las diferencias cuando se aplica una mayor exploración ($\\epsilon$ más alto) y una mayor explotación ($\\epsilon$ más bajo). ¿Cuál converge más rápido? ¿Cuál obtiene más valor? ¿Qué estrategia piensas que podría usarse para explorar y explotar de forma más inteligente?\n",
        "\n",
        "4.   Comentad las diferencias cuando se varían otros parámetros como el número de episodios, el ratio de aprendizaje \n",
        ". ¿Qué valores dan mejores resultados?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "sjfqAhjtaNQ1"
      },
      "outputs": [],
      "source": [
        "#Definimos estado inicial y actiones:\n",
        "S = (0, 0)\n",
        "actions = [(-1, 0), (1, 0), (0, -1), (0, 1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, compararemos las políticas de nuestros dos algoritmos en distintos mundos y parámetros:"
      ],
      "metadata": {
        "id": "ctaFv2dn7oe7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mundo 1 pequeño\n",
        "\n"
      ],
      "metadata": {
        "id": "w8oyUHD48Hdc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8vaeFlgVEQ1h",
        "outputId": "bac72f70-b2e9-47a0-c78e-022268043acb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mundo\n",
            "[ O  X  O  O  O \n",
            " O  X  O  X  O \n",
            " O  X  O  X  O \n",
            " O  X  O  X  O \n",
            " O  O  O  X  F ]\n",
            "\n",
            "Política\n",
            "\u001b[0m v \u001b[0m X \u001b[0m > \u001b[0m > \u001b[0m v \n",
            "\u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m v \n",
            "\u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m < \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m v \n",
            "\u001b[0m < \u001b[0m < \u001b[0m v \u001b[0m X \u001b[0m F \n",
            "\n",
            "[[1 0 3 3 1]\n",
            " [1 0 1 0 1]\n",
            " [1 0 0 0 1]\n",
            " [2 0 1 0 1]\n",
            " [2 2 1 0 0]]\n",
            "\n",
            "Episodios: 5000\n"
          ]
        }
      ],
      "source": [
        "policy_sarsa_1 = SARSA(w1p, actions, S, alpha=0.5, gamma=0.95, epsilon=0.1, max_iterations=5000, contador_max=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vWyfQedHp2m",
        "outputId": "80aa2ad6-82a4-4d41-ccc4-97015e2faaea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mundo\n",
            "[ O  X  O  O  O \n",
            " O  X  O  X  O \n",
            " O  X  O  X  O \n",
            " O  X  O  X  O \n",
            " O  O  O  X  F ]\n",
            "\n",
            "Política\n",
            "\u001b[0m v \u001b[0m X \u001b[0m > \u001b[0m > \u001b[0m v \n",
            "\u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m > \u001b[0m > \u001b[0m ^ \u001b[0m X \u001b[0m F \n",
            "\n",
            "[[1 0 3 3 1]\n",
            " [1 0 0 0 1]\n",
            " [1 0 0 0 1]\n",
            " [1 0 0 0 1]\n",
            " [3 3 0 0 0]]\n",
            "\n",
            "Episodios: 5000\n"
          ]
        }
      ],
      "source": [
        "policy_q_learning_1 = Q_Learning(w1p, actions, S, alpha=0.5, gamma=0.95, epsilon=0.1, max_iterations=5000, contador_max=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Podemos ver que utilizando los parámetros *alpha=0.5, gamma=0.95, epsilon=0.1, max_iterations=5000, contador_max=500*, solo el algoritmo **Q-Learning** logra encontrar el camino óptimo. A continuación, probaremos el algoritmo SARSA aumento a 800 el contador del segundo bucle, para ver si, teniendo más \"pasos\", logramos encontrar el camino óptimo."
      ],
      "metadata": {
        "id": "BcoPZm2GDj-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy_sarsa_1b = SARSA(w1p, actions, S, alpha=0.5, gamma=0.95, epsilon=0.1, max_iterations=5000, contador_max=800)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjKL25J5BbuS",
        "outputId": "e7b22dee-6b33-4a0d-8735-a77be80fb158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mundo\n",
            "[ O  X  O  O  O \n",
            " O  X  O  X  O \n",
            " O  X  O  X  O \n",
            " O  X  O  X  O \n",
            " O  O  O  X  F ]\n",
            "\n",
            "Política\n",
            "\u001b[0m v \u001b[0m X \u001b[0m > \u001b[0m > \u001b[0m v \n",
            "\u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m > \u001b[0m > \u001b[0m ^ \u001b[0m X \u001b[0m F \n",
            "\n",
            "[[1 0 3 3 1]\n",
            " [1 0 0 0 1]\n",
            " [1 0 0 0 1]\n",
            " [1 0 0 0 1]\n",
            " [3 3 0 0 0]]\n",
            "\n",
            "Episodios: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis Mundo 1 pequeño:** \n",
        "\n",
        "Con el aumento del parámetro *contador_max* en el algoritmo **SARSA** el agente ha encontrado el camino optimo. \n",
        "\n",
        "Podemos concluir que para el **mundo 1 pequeño**, el algoritmo que mejor resultados da es **Q-Learning**, ya que logra encuentra en menos iteraciones el camino óptimo. La principal diferencia entre los dos algoritmos es la actualización de Q, por tanto, probablemente este debe ser el motivo de que Q-Learning encuentra más rápido la solución en este tipo de mundo."
      ],
      "metadata": {
        "id": "rK9ohyIsDbzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mundo 1 mediano"
      ],
      "metadata": {
        "id": "Zyo6NRDRDLXm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQ6cpDT4QKwq",
        "outputId": "deeaf58a-1c88-4b2e-d572-d19e2276db81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mundo\n",
            "[ O  X  O  O  O  X  O  O  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  O  O  X  O  O  O  X  F ]\n",
            "\n",
            "Política\n",
            "\u001b[0m v \u001b[0m X \u001b[0m > \u001b[0m < \u001b[0m < \u001b[0m X \u001b[0m > \u001b[0m > \u001b[0m v \n",
            "\u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m v \n",
            "\u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m v \n",
            "\u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m > \n",
            "\u001b[0m < \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m v \n",
            "\u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m < \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m v \n",
            "\u001b[0m v \u001b[0m > \u001b[0m < \u001b[0m X \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m X \u001b[0m F \n",
            "\n",
            "[[1 0 3 2 2 0 3 3 1]\n",
            " [1 0 0 0 0 0 1 0 1]\n",
            " [1 0 0 0 1 0 1 0 1]\n",
            " [1 0 0 0 1 0 0 0 1]\n",
            " [1 0 0 0 0 0 1 0 3]\n",
            " [2 0 0 0 0 0 1 0 1]\n",
            " [1 0 1 0 0 0 0 0 1]\n",
            " [2 0 1 0 0 0 1 0 1]\n",
            " [1 3 2 0 1 1 1 0 0]]\n",
            "\n",
            "Episodios: 20000\n"
          ]
        }
      ],
      "source": [
        "policy_sarsa_1m = SARSA(w1m, actions, S, alpha=0.5, gamma=0.95, epsilon=0.1, max_iterations=20000, contador_max=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7a5whIYQOx5",
        "outputId": "af88d9a1-2b19-4029-a2cd-b6d663c0efb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mundo\n",
            "[ O  X  O  O  O  X  O  O  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  O  O  X  O  O  O  X  F ]\n",
            "\n",
            "Política\n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m X \u001b[0m > \u001b[0m > \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m v \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m v \u001b[0m ^ \u001b[0m X \u001b[0m F \n",
            "\n",
            "[[0 0 0 0 0 0 3 3 1]\n",
            " [0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 1]\n",
            " [0 1 0 0 0 1 0 0 0]]\n",
            "\n",
            "Episodios: 20000\n"
          ]
        }
      ],
      "source": [
        "policy_q_learning_1m = Q_Learning(w1m, actions, S, alpha=0.5, gamma=0.95, epsilon=0.1, max_iterations=20000, contador_max=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Podemos ver que para los parámetros *alpha=0.5, gamma=0.95, epsilon=0.1, max_iterations=20000, contador_max=1000* tanto SARSA como Q-Learning **no logran encontrar el valor óptimo**. A continuación, probaremos aumentar el número de iteraciones."
      ],
      "metadata": {
        "id": "0oh8Bnbo4ATO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy_sarsa_1m_2 = SARSA(w1m, actions, S, alpha=0.5, gamma=0.95, epsilon=0.1, max_iterations=30000, contador_max=3000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3g2TpP4-QaF",
        "outputId": "41b7f10f-5018-4c64-8ad7-0ac76b59d4db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mundo\n",
            "[ O  X  O  O  O  X  O  O  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  O  O  X  O  O  O  X  F ]\n",
            "\n",
            "Política\n",
            "\u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m < \u001b[0m < \u001b[0m X \u001b[0m > \u001b[0m > \u001b[0m v \n",
            "\u001b[0m < \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m < \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m < \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m > \n",
            "\u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m < \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m < \u001b[0m < \u001b[0m v \u001b[0m X \u001b[0m > \u001b[0m > \u001b[0m ^ \u001b[0m X \u001b[0m F \n",
            "\n",
            "[[1 0 0 2 2 0 3 3 1]\n",
            " [2 0 0 0 0 0 0 0 1]\n",
            " [2 0 0 0 0 0 0 0 1]\n",
            " [1 0 0 0 1 0 0 0 1]\n",
            " [2 0 0 0 1 0 0 0 3]\n",
            " [1 0 0 0 0 0 0 0 1]\n",
            " [1 0 0 0 1 0 0 0 1]\n",
            " [2 0 1 0 1 0 0 0 1]\n",
            " [2 2 1 0 3 3 0 0 0]]\n",
            "\n",
            "Episodios: 30000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy_q_learning_1m_2 = Q_Learning(w1m, actions, S, alpha=0.5, gamma=0.95, epsilon=0.1, max_iterations=30000, contador_max=3000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6sZ4D1k-Vmw",
        "outputId": "b85ce0f0-87d2-49fc-de75-1bbb80845e48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mundo\n",
            "[ O  X  O  O  O  X  O  O  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  O  O  X  O  O  O  X  F ]\n",
            "\n",
            "Política\n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m X \u001b[0m > \u001b[0m > \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m v \u001b[0m ^ \u001b[0m X \u001b[0m > \u001b[0m > \u001b[0m ^ \u001b[0m X \u001b[0m F \n",
            "\n",
            "[[0 0 0 0 0 0 3 3 1]\n",
            " [0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 1 0 0 0 1]\n",
            " [0 1 0 0 3 3 0 0 0]]\n",
            "\n",
            "Episodios: 30000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Podemos ver, principalmente en el algoritmo Q-Learning, que la política obtenida no cambia mucho al aumentar las iteraciones en ambos bucles, por lo que, probaremos si cambiando el valor de $\\epsilon$, ejemplo a 0.4, para que tenga opción de explorar más, nos da un mejor resultado."
      ],
      "metadata": {
        "id": "ehqY3NJC4kg2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy_sarsa_1m_3 = SARSA(w1m, actions, S, alpha=0.5, gamma=0.95, epsilon=0.4, max_iterations=20000, contador_max=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTVV-mSk2qzG",
        "outputId": "1fcd74c9-460a-40e6-a722-9b791c0a82db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mundo\n",
            "[ O  X  O  O  O  X  O  O  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  O  O  X  O  O  O  X  F ]\n",
            "\n",
            "Política\n",
            "\u001b[0m v \u001b[0m X \u001b[0m > \u001b[0m > \u001b[0m < \u001b[0m X \u001b[0m ^ \u001b[0m > \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m < \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m > \n",
            "\u001b[0m < \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m > \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m > \n",
            "\u001b[0m < \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m v \n",
            "\u001b[0m v \u001b[0m < \u001b[0m ^ \u001b[0m X \u001b[0m > \u001b[0m v \u001b[0m v \u001b[0m X \u001b[0m F \n",
            "\n",
            "[[1 0 3 3 2 0 0 3 1]\n",
            " [0 0 0 0 1 0 0 0 1]\n",
            " [2 0 0 0 0 0 0 0 1]\n",
            " [1 0 0 0 0 0 0 0 3]\n",
            " [2 0 0 0 0 0 1 0 3]\n",
            " [0 0 0 0 1 0 1 0 1]\n",
            " [0 0 1 0 0 0 0 0 3]\n",
            " [2 0 1 0 1 0 1 0 1]\n",
            " [1 2 0 0 3 1 1 0 0]]\n",
            "\n",
            "Episodios: 20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy_q_learning_1m_3 = Q_Learning(w1m, actions, S, alpha=0.5, gamma=0.95, epsilon=0.4, max_iterations=20000, contador_max=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_bzfGHs2v2u",
        "outputId": "bed9355f-0465-4bf8-badd-6edd84760f3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mundo\n",
            "[ O  X  O  O  O  X  O  O  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  O  O  X  O  O  O  X  F ]\n",
            "\n",
            "Política\n",
            "\u001b[0m v \u001b[0m X \u001b[0m > \u001b[0m > \u001b[0m v \u001b[0m X \u001b[0m > \u001b[0m > \u001b[0m v \n",
            "\u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m > \u001b[0m > \u001b[0m ^ \u001b[0m X \u001b[0m > \u001b[0m > \u001b[0m ^ \u001b[0m X \u001b[0m F \n",
            "\n",
            "[[1 0 3 3 1 0 3 3 1]\n",
            " [1 0 0 0 1 0 0 0 1]\n",
            " [1 0 0 0 1 0 0 0 1]\n",
            " [1 0 0 0 1 0 0 0 1]\n",
            " [1 0 0 0 1 0 0 0 1]\n",
            " [1 0 0 0 1 0 0 0 1]\n",
            " [1 0 0 0 1 0 0 0 1]\n",
            " [1 0 0 0 1 0 0 0 1]\n",
            " [3 3 0 0 3 3 0 0 0]]\n",
            "\n",
            "Episodios: 20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Con los parámetros utilizados inicialmente para el mundo 1 mediano, cambiando ahora el $\\epsilon$ a 0.4, hemos obtenido que el algoritmo Q-Learning encuentra el camino optimo. Ahora probaremos dejando el $\\epsilon$ en 0.4, aumentar el *contador_max* en el algoritmo SARSA, para ver si logramos que el agente encuentre el camino optimo.  "
      ],
      "metadata": {
        "id": "JeUg8lGU5n9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy_sarsa_1m_4 = SARSA(w1m, actions, S, alpha=0.5, gamma=0.95, epsilon=0.4, max_iterations=30000, contador_max=4500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7iBvtK-6h9r",
        "outputId": "51760450-2a78-41f5-9464-8cbd4594608d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mundo\n",
            "[ O  X  O  O  O  X  O  O  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  O  O  X  O  O  O  X  F ]\n",
            "\n",
            "Política\n",
            "\u001b[0m < \u001b[0m X \u001b[0m v \u001b[0m < \u001b[0m ^ \u001b[0m X \u001b[0m > \u001b[0m > \u001b[0m v \n",
            "\u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m > \n",
            "\u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m < \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m v \n",
            "\u001b[0m v \u001b[0m < \u001b[0m < \u001b[0m X \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m X \u001b[0m F \n",
            "\n",
            "[[2 0 1 2 0 0 3 3 1]\n",
            " [1 0 0 0 1 0 0 0 3]\n",
            " [1 0 0 0 1 0 0 0 1]\n",
            " [1 0 0 0 0 0 0 0 1]\n",
            " [2 0 0 0 0 0 0 0 1]\n",
            " [0 0 1 0 0 0 0 0 1]\n",
            " [1 0 0 0 1 0 0 0 1]\n",
            " [1 0 0 0 1 0 1 0 1]\n",
            " [1 2 2 0 1 1 1 0 0]]\n",
            "\n",
            "Episodios: 30000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Hemos probado varios valores aumentando los parámetros de iteración y no hemos conseguido que el agente encuentre la solución óptima. Ahora intentaremos bajando el épsilon a 0.01."
      ],
      "metadata": {
        "id": "jCffvgDw-kkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy_sarsa_1m_5 = SARSA(w1m, actions, S, alpha=0.5, gamma=0.95, epsilon=0.01, max_iterations=20000, contador_max=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sib8_THE_WKe",
        "outputId": "2eaa905f-d4b6-45ae-8e12-7695fb08d4dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mundo\n",
            "[ O  X  O  O  O  X  O  O  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O \n",
            " O  O  O  X  O  O  O  X  F ]\n",
            "\n",
            "Política\n",
            "\u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m < \u001b[0m < \u001b[0m X \u001b[0m ^ \u001b[0m ^ \u001b[0m v \n",
            "\u001b[0m < \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m < \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m < \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m < \u001b[0m < \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m v \u001b[0m v \u001b[0m X \u001b[0m F \n",
            "\n",
            "[[1 0 0 2 2 0 0 0 1]\n",
            " [2 0 0 0 0 0 0 0 1]\n",
            " [1 0 0 0 0 0 0 0 1]\n",
            " [2 0 0 0 0 0 0 0 1]\n",
            " [2 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 1]\n",
            " [2 2 0 0 0 1 1 0 0]]\n",
            "\n",
            "Episodios: 20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* A simple vista, podemos ver que utilizar un épsilon pequeño, para que explote más, no es recomendable en este tipo de mundos con laberintos."
      ],
      "metadata": {
        "id": "P5LzJS4_AL9o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis Mundo 1 mediano:** \n",
        "\n",
        "Podemos concluir que para el mundo 1 mediado, que es un laberinto con camino único, el algoritmo que mejor funciona para que el agente encuentre el camino optimo es **Q-Learning**, pero utilizando un épsilon más exploratorio, es este caso hemos probado con un 40%."
      ],
      "metadata": {
        "id": "ShdzPcwOGC_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mundo 2 pequeño"
      ],
      "metadata": {
        "id": "AaG6QkXCD018"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy_sarsa_2p = SARSA(w2p, actions, S, alpha=0.5, gamma=0.9, epsilon=0.1, max_iterations=2000, contador_max=500)"
      ],
      "metadata": {
        "id": "mdXPgXpu59Q3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1dc0aa44-ac74-4995-a4f7-1ffbeadb00ac"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mundo\n",
            "[ O  O  O  O  O \n",
            " O  O  O  O  O \n",
            " T  O  O  X  O \n",
            " O  O  X  X  O \n",
            " O  O  T  O  F ]\n",
            "\n",
            "Política\n",
            "\u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \n",
            "\u001b[0m v \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m v \n",
            "\u001b[;36;47m v \u001b[0m < \u001b[0m < \u001b[0m X \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m < \u001b[0m X \u001b[0m X \u001b[0m v \n",
            "\u001b[0m > \u001b[0m > \u001b[;36;47m > \u001b[0m > \u001b[0m F \n",
            "\n",
            "[[1 1 1 1 1]\n",
            " [1 3 3 3 1]\n",
            " [1 2 2 0 1]\n",
            " [0 2 0 0 1]\n",
            " [3 3 3 3 0]]\n",
            "\n",
            "Episodios: 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy_q_learning_2p = Q_Learning(w2p, actions, S, alpha=0.5, gamma=0.9, epsilon=0.1, max_iterations=2000, contador_max=500)"
      ],
      "metadata": {
        "id": "uXYnREcX6AZQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "933ebc23-4828-4922-e7ed-79bb30b76adf"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mundo\n",
            "[ O  O  O  O  O \n",
            " O  O  O  O  O \n",
            " T  O  O  X  O \n",
            " O  O  X  X  O \n",
            " O  O  T  O  F ]\n",
            "\n",
            "Política\n",
            "\u001b[0m v \u001b[0m v \u001b[0m > \u001b[0m v \u001b[0m v \n",
            "\u001b[0m v \u001b[0m v \u001b[0m > \u001b[0m > \u001b[0m v \n",
            "\u001b[;36;47m v \u001b[0m < \u001b[0m < \u001b[0m X \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m v \u001b[0m X \u001b[0m X \u001b[0m v \n",
            "\u001b[0m > \u001b[0m > \u001b[;36;47m > \u001b[0m > \u001b[0m F \n",
            "\n",
            "[[1 1 3 1 1]\n",
            " [1 1 3 3 1]\n",
            " [1 2 2 0 1]\n",
            " [0 1 0 0 1]\n",
            " [3 3 3 3 0]]\n",
            "\n",
            "Episodios: 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Aumentamos el valor de épsilon. Al ser un mundo con más caminos posibles que el mundo 1, favorecer la exploración debería ayudar a encontrar la solución óptima. Aumentamos el número de iteraciones también para compensar."
      ],
      "metadata": {
        "id": "tq2w_YrwpcnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy_sarsa_2p_2 = SARSA(w2p, actions, S, alpha=0.5, gamma=0.9, epsilon=0.3, max_iterations=2000, contador_max=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKdDNhyUo7p3",
        "outputId": "aa321b23-ae58-48d1-a197-47f9186dbfc5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mundo\n",
            "[ O  O  O  O  O \n",
            " O  O  O  O  O \n",
            " T  O  O  X  O \n",
            " O  O  X  X  O \n",
            " O  O  T  O  F ]\n",
            "\n",
            "Política\n",
            "\u001b[0m v \u001b[0m < \u001b[0m > \u001b[0m v \u001b[0m v \n",
            "\u001b[0m v \u001b[0m < \u001b[0m < \u001b[0m > \u001b[0m v \n",
            "\u001b[;36;47m v \u001b[0m < \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m < \u001b[0m X \u001b[0m X \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m > \u001b[;36;47m > \u001b[0m > \u001b[0m F \n",
            "\n",
            "[[1 2 3 1 1]\n",
            " [1 2 2 3 1]\n",
            " [1 2 0 0 1]\n",
            " [0 2 0 0 1]\n",
            " [0 3 3 3 0]]\n",
            "\n",
            "Episodios: 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy_q_learning_2p_2 = Q_Learning(w2p, actions, S, alpha=0.5, gamma=0.9, epsilon=0.3, max_iterations=2000, contador_max=500)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fU_wC10pO12",
        "outputId": "aefe60a2-b2cb-45f4-f42a-02e11b870dfc"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mundo\n",
            "[ O  O  O  O  O \n",
            " O  O  O  O  O \n",
            " T  O  O  X  O \n",
            " O  O  X  X  O \n",
            " O  O  T  O  F ]\n",
            "\n",
            "Política\n",
            "\u001b[0m v \u001b[0m v \u001b[0m > \u001b[0m v \u001b[0m v \n",
            "\u001b[0m v \u001b[0m v \u001b[0m > \u001b[0m > \u001b[0m v \n",
            "\u001b[;36;47m > \u001b[0m < \u001b[0m < \u001b[0m X \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m v \u001b[0m X \u001b[0m X \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m > \u001b[;36;47m > \u001b[0m > \u001b[0m F \n",
            "\n",
            "[[1 1 3 1 1]\n",
            " [1 1 3 3 1]\n",
            " [3 2 2 0 1]\n",
            " [0 1 0 0 1]\n",
            " [0 3 3 3 0]]\n",
            "\n",
            "Episodios: 2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis Mundo 2 pequeño:** \n",
        "\n",
        "Para este mundo, ambos algoritmos funcionan de una manera muy eficiente para un número pequeño de iteraciones, si bien el contador del segundo bucle se debe mantener relativamente alto para garanizar que explore los caminos. De nuevo, Q-learning funciona mejor que SARSA, pues en para el mismo número de pasos sí consigue encontrar la solución más cercana a la óptima."
      ],
      "metadata": {
        "id": "N88bbVgxm-AG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mundo 2 mediano\n"
      ],
      "metadata": {
        "id": "VwT1dUhim-Dz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy_sarsa_2m = SARSA(w2m, actions, S, alpha=0.5, gamma=0.9, epsilon=0.1, max_iterations=10000, contador_max=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PLcxtujpDX8Y",
        "outputId": "4bdd0f6e-4281-4415-a2b3-ade895dfcf76"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mundo\n",
            "[ O  O  O  O  O  O  O  O  O  O \n",
            " O  X  X  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O \n",
            " O  T  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  X  O  X  O  O \n",
            " O  O  X  O  O  O  O  O  O  O \n",
            " O  O  O  X  O  O  O  O  O  O \n",
            " O  X  O  X  O  O  O  O  O  O \n",
            " O  O  O  X  O  O  T  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  F ]\n",
            "\n",
            "Política\n",
            "\u001b[0m v \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m v \u001b[0m v \u001b[0m > \u001b[0m v \u001b[0m v \n",
            "\u001b[0m v \u001b[0m X \u001b[0m X \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m v \u001b[0m v \u001b[0m v \n",
            "\u001b[0m > \u001b[0m v \u001b[0m < \u001b[0m v \u001b[0m v \u001b[0m > \u001b[0m > \u001b[0m v \u001b[0m v \u001b[0m v \n",
            "\u001b[0m > \u001b[;36;47m ^ \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m v \n",
            "\u001b[0m > \u001b[0m ^ \u001b[0m < \u001b[0m v \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m ^ \u001b[0m X \u001b[0m > \u001b[0m v \u001b[0m v \u001b[0m > \u001b[0m v \u001b[0m v \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m < \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m > \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m v \u001b[0m v \u001b[0m X \u001b[0m > \u001b[0m > \u001b[;36;47m > \u001b[0m > \u001b[0m v \u001b[0m v \n",
            "\u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m F \n",
            "\n",
            "[[1 3 3 3 3 1 1 3 1 1]\n",
            " [1 0 0 3 3 3 3 1 1 1]\n",
            " [3 1 2 1 1 3 3 1 1 1]\n",
            " [3 0 1 1 1 3 3 3 3 1]\n",
            " [3 0 2 1 1 0 1 0 1 1]\n",
            " [0 0 0 3 1 1 3 1 1 1]\n",
            " [0 2 1 0 1 1 1 1 1 1]\n",
            " [0 0 1 0 3 1 1 1 1 1]\n",
            " [0 1 1 0 3 3 3 3 1 1]\n",
            " [3 3 3 3 3 3 3 3 3 0]]\n",
            "\n",
            "Episodios: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy_q_learning_2m_2 = Q_Learning(w2m, actions, S, alpha=0.5, gamma=0.9, epsilon=0.1, max_iterations=20000, contador_max=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q7MS_BmADcRU",
        "outputId": "708f0b26-26af-48b0-f563-49f2a3b567fc"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mundo\n",
            "[ O  O  O  O  O  O  O  O  O  O \n",
            " O  X  X  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O \n",
            " O  T  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  X  O  X  O  O \n",
            " O  O  X  O  O  O  O  O  O  O \n",
            " O  O  O  X  O  O  O  O  O  O \n",
            " O  X  O  X  O  O  O  O  O  O \n",
            " O  O  O  X  O  O  T  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  F ]\n",
            "\n",
            "Política\n",
            "\u001b[0m v \u001b[0m < \u001b[0m < \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \n",
            "\u001b[0m v \u001b[0m X \u001b[0m X \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \n",
            "\u001b[0m v \u001b[0m v \u001b[0m < \u001b[0m < \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \n",
            "\u001b[0m > \u001b[;36;47m v \u001b[0m ^ \u001b[0m > \u001b[0m v \u001b[0m > \u001b[0m v \u001b[0m > \u001b[0m v \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m ^ \u001b[0m < \u001b[0m v \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m ^ \u001b[0m X \u001b[0m > \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m ^ \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m v \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m v \u001b[;36;47m v \u001b[0m v \u001b[0m v \u001b[0m v \n",
            "\u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m F \n",
            "\n",
            "[[1 2 2 1 1 1 1 1 1 1]\n",
            " [1 0 0 1 1 1 1 1 1 1]\n",
            " [1 1 2 2 1 1 1 1 1 1]\n",
            " [3 1 0 3 1 3 1 3 1 1]\n",
            " [0 0 2 1 1 0 1 0 1 1]\n",
            " [0 0 0 3 1 1 1 1 1 1]\n",
            " [0 0 1 0 1 1 1 1 1 1]\n",
            " [0 0 1 0 1 1 1 1 1 1]\n",
            " [0 1 1 0 1 1 1 1 1 1]\n",
            " [3 3 3 3 3 3 3 3 3 0]]\n",
            "\n",
            "Episodios: 20000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis Mundo 2 mediano:** \n",
        "\n",
        "Para este mundo, el número de iteraciones requeridas por Q-learning para dar resultados aceptables es consirablemente superior al de SARSA. Es ambos casos, alcanzar la solución óptima exige un número de pasos muy elevado."
      ],
      "metadata": {
        "id": "Dma8hfAtGHMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mundo 3 mediano "
      ],
      "metadata": {
        "id": "nVd2nhKdEBtT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* En principio probaremos con los mismos parámetros utilizado en el mundo 1 mediano, dependiendo de su resultado, intentaremos subir o bajar las iteraciones."
      ],
      "metadata": {
        "id": "WBa6hiNhG_Bj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1DHqaMkQaac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a264b59-2651-4806-e840-f00addc674c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mundo\n",
            "[ O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  T  O \n",
            " O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O \n",
            " O  O  X  O  O  O  O  O  O  O \n",
            " O  O  O  X  O  O  O  O  O  O \n",
            " O  O  X  O  O  O  O  X  O  O \n",
            " O  O  X  O  O  O  O  O  O  O \n",
            " O  T  X  O  O  O  O  X  O  O \n",
            " O  O  O  O  O  O  O  O  O  F ]\n",
            "\n",
            "Política\n",
            "\u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m > \u001b[0m v \u001b[0m > \u001b[0m v \u001b[0m > \u001b[0m v \n",
            "\u001b[0m v \u001b[0m v \u001b[0m > \u001b[0m > \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[;36;47m v \u001b[0m v \n",
            "\u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m v \u001b[0m v \u001b[0m > \u001b[0m v \u001b[0m v \n",
            "\u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m v \u001b[0m v \u001b[0m > \u001b[0m v \u001b[0m v \n",
            "\u001b[0m v \u001b[0m ^ \u001b[0m X \u001b[0m > \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m > \u001b[0m v \u001b[0m v \n",
            "\u001b[0m v \u001b[0m v \u001b[0m < \u001b[0m X \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m v \u001b[0m v \n",
            "\u001b[0m v \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m v \n",
            "\u001b[0m v \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m > \u001b[0m > \u001b[0m v \u001b[0m v \n",
            "\u001b[0m > \u001b[;36;47m v \u001b[0m X \u001b[0m > \u001b[0m > \u001b[0m v \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m v \n",
            "\u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m F \n",
            "\n",
            "[[1 1 1 1 3 1 3 1 3 1]\n",
            " [1 1 3 3 1 1 1 1 1 1]\n",
            " [3 3 3 3 3 1 1 3 1 1]\n",
            " [3 3 3 3 3 1 1 3 1 1]\n",
            " [1 0 0 3 1 1 1 3 1 1]\n",
            " [1 1 2 0 3 3 3 3 1 1]\n",
            " [1 1 0 1 1 1 1 0 1 1]\n",
            " [1 1 0 1 1 1 3 3 1 1]\n",
            " [3 1 0 3 3 1 1 0 1 1]\n",
            " [3 3 3 3 3 3 3 3 3 0]]\n",
            "\n",
            "Episodios: 20000\n"
          ]
        }
      ],
      "source": [
        "policy_sarsa_3m = SARSA(w3m, actions, S,  alpha=0.5, gamma=0.95, epsilon=0.1, max_iterations=20000, contador_max=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7HSgIWaZQV0v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "48ed5b50-8312-42bf-ba4f-7eb4a748a571"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mundo\n",
            "[ O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  T  O \n",
            " O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O \n",
            " O  O  X  O  O  O  O  O  O  O \n",
            " O  O  O  X  O  O  O  O  O  O \n",
            " O  O  X  O  O  O  O  X  O  O \n",
            " O  O  X  O  O  O  O  O  O  O \n",
            " O  T  X  O  O  O  O  X  O  O \n",
            " O  O  O  O  O  O  O  O  O  F ]\n",
            "\n",
            "Política\n",
            "\u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m > \u001b[0m > \u001b[0m v \u001b[0m > \u001b[0m > \u001b[0m v \u001b[0m v \n",
            "\u001b[0m v \u001b[0m v \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[;36;47m > \u001b[0m v \n",
            "\u001b[0m v \u001b[0m v \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m v \n",
            "\u001b[0m v \u001b[0m v \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m v \n",
            "\u001b[0m v \u001b[0m v \u001b[0m X \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m v \n",
            "\u001b[0m v \u001b[0m v \u001b[0m < \u001b[0m X \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m v \n",
            "\u001b[0m v \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m X \u001b[0m > \u001b[0m v \n",
            "\u001b[0m v \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m > \u001b[0m > \u001b[0m v \n",
            "\u001b[0m v \u001b[;36;47m v \u001b[0m X \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m v \n",
            "\u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m F \n",
            "\n",
            "[[1 1 1 3 3 1 3 3 1 1]\n",
            " [1 1 3 3 3 3 3 3 3 1]\n",
            " [1 1 3 3 3 3 3 3 3 1]\n",
            " [1 1 3 3 3 3 3 3 3 1]\n",
            " [1 1 0 3 3 3 3 3 3 1]\n",
            " [1 1 2 0 3 3 3 3 3 1]\n",
            " [1 1 0 1 1 1 1 0 3 1]\n",
            " [1 1 0 1 1 1 1 3 3 1]\n",
            " [1 1 0 1 1 1 1 0 1 1]\n",
            " [3 3 3 3 3 3 3 3 3 0]]\n",
            "\n",
            "Episodios: 20000\n"
          ]
        }
      ],
      "source": [
        "policy_q_learning_3m = Q_Learning(w3m, actions, S, alpha=0.5, gamma=0.95, epsilon=0.1, max_iterations=20000, contador_max=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Podemos observar que ambos logran llegar a término si nos paramos en cualquier punto del mundo, pero con políticas distintas. Ahora intentaremos probar con menos iteraciones en ambos algoritmos a ver si llegan a término. "
      ],
      "metadata": {
        "id": "qAVrKgM6Hn_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy_sarsa_3m_2 = SARSA(w3m, actions, S,  alpha=0.5, gamma=0.95, epsilon=0.1, max_iterations=10000, contador_max=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUFU02MdFMjZ",
        "outputId": "f66d5bdc-c913-4f71-e93e-bd68252536b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mundo\n",
            "[ O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  T  O \n",
            " O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O \n",
            " O  O  X  O  O  O  O  O  O  O \n",
            " O  O  O  X  O  O  O  O  O  O \n",
            " O  O  X  O  O  O  O  X  O  O \n",
            " O  O  X  O  O  O  O  O  O  O \n",
            " O  T  X  O  O  O  O  X  O  O \n",
            " O  O  O  O  O  O  O  O  O  F ]\n",
            "\n",
            "Política\n",
            "\u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \n",
            "\u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m v \u001b[0m v \u001b[;36;47m v \u001b[0m v \n",
            "\u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m v \u001b[0m v \n",
            "\u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m v \u001b[0m > \u001b[0m v \u001b[0m > \u001b[0m > \u001b[0m v \u001b[0m v \n",
            "\u001b[0m v \u001b[0m v \u001b[0m X \u001b[0m > \u001b[0m v \u001b[0m v \u001b[0m > \u001b[0m > \u001b[0m v \u001b[0m v \n",
            "\u001b[0m v \u001b[0m v \u001b[0m < \u001b[0m X \u001b[0m > \u001b[0m v \u001b[0m > \u001b[0m > \u001b[0m v \u001b[0m v \n",
            "\u001b[0m v \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m > \u001b[0m v \u001b[0m v \u001b[0m X \u001b[0m > \u001b[0m v \n",
            "\u001b[0m > \u001b[0m < \u001b[0m X \u001b[0m v \u001b[0m > \u001b[0m v \u001b[0m > \u001b[0m > \u001b[0m v \u001b[0m v \n",
            "\u001b[0m v \u001b[;36;47m v \u001b[0m X \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m X \u001b[0m > \u001b[0m v \n",
            "\u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m F \n",
            "\n",
            "[[3 3 3 3 3 3 1 1 1 1]\n",
            " [3 3 3 3 3 3 1 1 1 1]\n",
            " [3 3 3 3 3 3 3 3 1 1]\n",
            " [3 3 3 1 3 1 3 3 1 1]\n",
            " [1 1 0 3 1 1 3 3 1 1]\n",
            " [1 1 2 0 3 1 3 3 1 1]\n",
            " [1 1 0 1 3 1 1 0 3 1]\n",
            " [3 2 0 1 3 1 3 3 1 1]\n",
            " [1 1 0 1 1 1 1 0 3 1]\n",
            " [3 3 3 3 3 3 3 3 3 0]]\n",
            "\n",
            "Episodios: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy_q_learning_3m_2 = Q_Learning(w3m, actions, S, alpha=0.5, gamma=0.95, epsilon=0.1, max_iterations=10000, contador_max=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLRkbvTSFUhM",
        "outputId": "f404b276-28db-46f7-ec91-c94c1f8a9cdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mundo\n",
            "[ O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  T  O \n",
            " O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O \n",
            " O  O  X  O  O  O  O  O  O  O \n",
            " O  O  O  X  O  O  O  O  O  O \n",
            " O  O  X  O  O  O  O  X  O  O \n",
            " O  O  X  O  O  O  O  O  O  O \n",
            " O  T  X  O  O  O  O  X  O  O \n",
            " O  O  O  O  O  O  O  O  O  F ]\n",
            "\n",
            "Política\n",
            "\u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \n",
            "\u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[;36;47m ^ \u001b[0m ^ \n",
            "\u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \n",
            "\u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \n",
            "\u001b[0m ^ \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \n",
            "\u001b[0m ^ \u001b[0m ^ \u001b[0m < \u001b[0m X \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \n",
            "\u001b[0m ^ \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m ^ \n",
            "\u001b[0m ^ \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m < \u001b[0m ^ \u001b[0m ^ \n",
            "\u001b[0m ^ \u001b[;36;47m ^ \u001b[0m X \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m ^ \n",
            "\u001b[0m ^ \u001b[0m ^ \u001b[0m v \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m v \u001b[0m ^ \u001b[0m F \n",
            "\n",
            "[[0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 2 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 2 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 1 0 0]]\n",
            "\n",
            "Episodios: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Utilizado menos internaciones en bucle global, Q-Learning no logra encontrar el camino optimo, y SARSA sí logra llegar en la mayor de estados y puntos del mundo, pero no en todos. Probaremos ahora utilizando menos iteraciones que la primera prueba, pero ahora con un $\\epsilon$ de 0.5, para ver si aumentado la exploración logra encontrar el camino óptimo."
      ],
      "metadata": {
        "id": "QX2RzZ9-cL-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "policy_sarsa_3m_3 = SARSA(w3m, actions, S,  alpha=0.5, gamma=0.95, epsilon=0.5, max_iterations=10000, contador_max=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdU2v5CrHr-5",
        "outputId": "fecc5eca-cbfb-4bfa-8a5c-2ef19fac54df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mundo\n",
            "[ O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  T  O \n",
            " O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O \n",
            " O  O  X  O  O  O  O  O  O  O \n",
            " O  O  O  X  O  O  O  O  O  O \n",
            " O  O  X  O  O  O  O  X  O  O \n",
            " O  O  X  O  O  O  O  O  O  O \n",
            " O  T  X  O  O  O  O  X  O  O \n",
            " O  O  O  O  O  O  O  O  O  F ]\n",
            "\n",
            "Política\n",
            "\u001b[0m > \u001b[0m v \u001b[0m > \u001b[0m > \u001b[0m v \u001b[0m > \u001b[0m v \u001b[0m < \u001b[0m < \u001b[0m > \n",
            "\u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m v \u001b[0m v \u001b[;36;47m v \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m > \u001b[0m > \u001b[0m ^ \u001b[0m > \u001b[0m v \u001b[0m > \u001b[0m v \u001b[0m v \u001b[0m v \n",
            "\u001b[0m > \u001b[0m ^ \u001b[0m ^ \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m v \u001b[0m v \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m < \u001b[0m X \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m ^ \u001b[0m > \u001b[0m v \u001b[0m v \n",
            "\u001b[0m v \u001b[0m < \u001b[0m < \u001b[0m X \u001b[0m v \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m v \u001b[0m v \n",
            "\u001b[0m v \u001b[0m < \u001b[0m X \u001b[0m v \u001b[0m v \u001b[0m < \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m v \n",
            "\u001b[0m v \u001b[0m v \u001b[0m X \u001b[0m > \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m > \u001b[0m v \u001b[0m v \n",
            "\u001b[0m v \u001b[;36;47m < \u001b[0m X \u001b[0m > \u001b[0m > \u001b[0m ^ \u001b[0m v \u001b[0m X \u001b[0m > \u001b[0m v \n",
            "\u001b[0m > \u001b[0m < \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m F \n",
            "\n",
            "[[3 1 3 3 1 3 1 2 2 3]\n",
            " [3 3 3 3 3 3 1 1 1 1]\n",
            " [0 3 3 0 3 1 3 1 1 1]\n",
            " [3 0 0 3 3 3 3 1 1 1]\n",
            " [0 2 0 3 3 3 0 3 1 1]\n",
            " [1 2 2 0 1 3 3 3 1 1]\n",
            " [1 2 0 1 1 2 1 0 1 1]\n",
            " [1 1 0 3 1 1 1 3 1 1]\n",
            " [1 2 0 3 3 0 1 0 3 1]\n",
            " [3 2 3 3 3 3 3 3 3 0]]\n",
            "\n",
            "Episodios: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy_q_learning_3m_3 = Q_Learning(w3m, actions, S, alpha=0.5, gamma=0.95, epsilon=0.5, max_iterations=10000, contador_max=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90eViOcwHuCW",
        "outputId": "d05446fe-9408-4db5-e7ae-90108504d573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mundo\n",
            "[ O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  T  O \n",
            " O  O  O  O  O  O  O  O  O  O \n",
            " O  O  O  O  O  O  O  O  O  O \n",
            " O  O  X  O  O  O  O  O  O  O \n",
            " O  O  O  X  O  O  O  O  O  O \n",
            " O  O  X  O  O  O  O  X  O  O \n",
            " O  O  X  O  O  O  O  O  O  O \n",
            " O  T  X  O  O  O  O  X  O  O \n",
            " O  O  O  O  O  O  O  O  O  F ]\n",
            "\n",
            "Política\n",
            "\u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \n",
            "\u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m > \u001b[0m v \u001b[;36;47m > \u001b[0m v \n",
            "\u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \n",
            "\u001b[0m v \u001b[0m v \u001b[0m > \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \n",
            "\u001b[0m v \u001b[0m v \u001b[0m X \u001b[0m > \u001b[0m v \u001b[0m v \u001b[0m > \u001b[0m v \u001b[0m v \u001b[0m v \n",
            "\u001b[0m v \u001b[0m v \u001b[0m < \u001b[0m X \u001b[0m v \u001b[0m v \u001b[0m > \u001b[0m > \u001b[0m v \u001b[0m v \n",
            "\u001b[0m v \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m v \n",
            "\u001b[0m v \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m > \u001b[0m v \u001b[0m v \n",
            "\u001b[0m v \u001b[;36;47m v \u001b[0m X \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m v \n",
            "\u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m F \n",
            "\n",
            "[[1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1 3 1 3 1]\n",
            " [1 1 1 1 1 1 1 1 1 1]\n",
            " [1 1 3 1 1 1 1 1 1 1]\n",
            " [1 1 0 3 1 1 3 1 1 1]\n",
            " [1 1 2 0 1 1 3 3 1 1]\n",
            " [1 1 0 1 1 1 1 0 1 1]\n",
            " [1 1 0 1 1 1 1 3 1 1]\n",
            " [1 1 0 1 1 1 1 0 1 1]\n",
            " [3 3 3 3 3 3 3 3 3 0]]\n",
            "\n",
            "Episodios: 10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Con un valor de 0.5 en el épsilon Q-Learning logra entontar el camino optimo, pero Sarsa a simple vista no mejora, ya que no llega a término en alguno puntos del mundo. "
      ],
      "metadata": {
        "id": "azFWl92Fd7BY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis Mundo 3 mediano:** \n",
        "\n",
        "Podemos concluir que en el mundo 3 mediano, el algoritmo que mejor funciona es **Q-Learning**, aunque hay que tener en cuenta a que menor iteraciones es necesario un épsilon más exploratorio."
      ],
      "metadata": {
        "id": "C-ZnSV3rGLsg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mundo 4 "
      ],
      "metadata": {
        "id": "Qp6QKmkOEHcs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFUR2QbcmDbc",
        "outputId": "a77769e3-d19f-4b07-da7f-ac1a98f4878e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mundo\n",
            "[ O  X  O  X  O  O  O  O  O  X  O  O  O  O  O  X  X  X  O  X  O \n",
            " O  X  O  X  X  X  X  X  O  X  X  X  X  X  O  O  O  X  O  X  O \n",
            " O  X  O  O  O  O  O  O  O  X  O  O  O  X  O  X  X  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O  X  O  X  O  O  O  O  X  O  O  X  O \n",
            " O  O  O  X  O  X  O  X  X  X  X  X  X  X  X  O  X  O  X  X  O \n",
            " X  X  X  X  O  X  O  O  O  X  O  O  O  O  O  O  X  O  O  O  O \n",
            " O  O  O  O  O  X  X  X  O  X  X  X  X  X  X  O  X  X  O  X  O \n",
            " X  X  X  X  O  X  O  X  O  X  O  O  O  O  O  O  O  O  O  X  O \n",
            " O  O  O  X  O  O  O  X  X  X  O  O  X  X  X  X  X  X  X  X  O \n",
            " O  X  O  X  O  X  O  X  O  O  O  X  X  O  O  O  O  O  O  X  X \n",
            " O  X  O  X  O  X  X  X  O  X  O  X  O  O  X  X  X  X  O  O  O \n",
            " O  X  O  X  O  X  O  O  O  X  O  X  O  X  X  O  O  X  X  X  O \n",
            " O  X  O  O  O  X  X  O  X  X  O  X  O  X  O  O  O  O  O  X  O \n",
            " O  X  X  X  X  X  O  O  X  O  O  O  O  O  O  X  X  X  O  X  O \n",
            " O  O  O  O  X  O  O  X  X  O  X  O  X  X  O  X  O  O  O  X  O \n",
            " X  X  X  O  O  O  X  X  O  O  X  O  O  X  X  X  O  X  X  X  X \n",
            " O  O  X  X  O  X  X  X  X  X  X  X  O  O  O  X  O  X  O  O  O \n",
            " X  O  O  X  O  X  O  O  O  X  O  O  O  X  X  X  O  X  O  X  O \n",
            " X  X  O  O  O  X  X  X  O  X  X  X  O  O  O  X  O  O  O  X  O \n",
            " O  X  X  O  X  X  O  O  O  O  O  X  O  X  X  X  X  X  X  X  O \n",
            " O  O  O  O  O  O  O  X  X  X  O  X  O  O  O  O  O  O  O  X  F ]\n",
            "\n",
            "Política\n",
            "\u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m < \u001b[0m ^ \u001b[0m ^ \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m ^ \u001b[0m > \u001b[0m < \u001b[0m v \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m v \n",
            "\u001b[0m < \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m < \u001b[0m < \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \n",
            "\u001b[0m < \u001b[0m X \u001b[0m ^ \u001b[0m < \u001b[0m v \u001b[0m < \u001b[0m v \u001b[0m < \u001b[0m ^ \u001b[0m X \u001b[0m > \u001b[0m ^ \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m < \u001b[0m ^ \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m ^ \u001b[0m X \u001b[0m ^ \n",
            "\u001b[0m < \u001b[0m > \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m ^ \n",
            "\u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m < \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m < \u001b[0m ^ \u001b[0m < \u001b[0m > \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m v \u001b[0m < \u001b[0m ^ \n",
            "\u001b[0m > \u001b[0m < \u001b[0m < \u001b[0m < \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \n",
            "\u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m < \u001b[0m < \u001b[0m < \u001b[0m < \u001b[0m ^ \u001b[0m < \u001b[0m < \u001b[0m ^ \u001b[0m X \u001b[0m ^ \n",
            "\u001b[0m v \u001b[0m < \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m < \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m < \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m v \u001b[0m < \u001b[0m < \u001b[0m < \u001b[0m ^ \u001b[0m v \u001b[0m X \u001b[0m X \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m < \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m > \u001b[0m v \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m v \u001b[0m v \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m < \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m ^ \u001b[0m ^ \u001b[0m < \u001b[0m v \u001b[0m X \u001b[0m ^ \n",
            "\u001b[0m < \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m > \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m ^ \u001b[0m < \u001b[0m ^ \u001b[0m < \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \n",
            "\u001b[0m > \u001b[0m ^ \u001b[0m < \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m < \u001b[0m ^ \u001b[0m X \u001b[0m ^ \n",
            "\u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m > \u001b[0m v \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m > \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m v \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \n",
            "\u001b[0m > \u001b[0m < \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m v \u001b[0m < \u001b[0m < \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m > \u001b[0m < \u001b[0m v \n",
            "\u001b[0m X \u001b[0m ^ \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m > \u001b[0m < \u001b[0m < \u001b[0m X \u001b[0m > \u001b[0m < \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m < \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m < \u001b[0m < \u001b[0m X \u001b[0m ^ \u001b[0m > \u001b[0m ^ \u001b[0m X \u001b[0m v \n",
            "\u001b[0m v \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m > \u001b[0m ^ \u001b[0m ^ \u001b[0m < \u001b[0m < \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m v \n",
            "\u001b[0m < \u001b[0m < \u001b[0m < \u001b[0m ^ \u001b[0m < \u001b[0m < \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m < \u001b[0m < \u001b[0m > \u001b[0m > \u001b[0m > \u001b[0m v \u001b[0m X \u001b[0m F \n",
            "\n",
            "[[1 0 1 0 0 2 0 0 1 0 0 0 3 2 1 0 0 0 1 0 1]\n",
            " [2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0 0 0 0]\n",
            " [2 0 0 2 1 2 1 2 0 0 3 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1 0 1 0 0 0]\n",
            " [2 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 2 1 0 0 2 0 2 3 0 0 0 1 2 0]\n",
            " [3 2 2 2 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 1 0 0 0 1 2 2 2 2 0 2 2 0 0 0]\n",
            " [1 2 1 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 2 0 0 0 1 2 2 2 0 1 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 2 1]\n",
            " [0 0 0 0 0 0 3 1 0 0 0 0 0 0 0 1 1 0 0 0 1]\n",
            " [0 0 0 2 0 0 0 0 0 0 0 0 0 0 1 0 0 2 1 0 0]\n",
            " [2 0 0 0 0 0 3 0 0 1 0 2 0 2 0 0 0 0 0 0 0]\n",
            " [3 0 2 1 0 1 0 0 0 0 0 0 0 0 0 0 1 2 0 0 0]\n",
            " [0 0 0 3 1 0 0 0 3 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [3 2 0 0 0 0 0 0 0 0 0 0 1 2 2 0 0 0 3 2 1]\n",
            " [0 0 0 0 0 0 3 2 2 0 3 2 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 2 0 0 0 0 0 0 0 0 0 2 2 0 0 3 0 0 1]\n",
            " [1 0 0 0 0 0 3 0 0 2 2 0 0 0 0 0 0 0 0 0 1]\n",
            " [2 2 2 0 2 2 0 0 0 0 0 0 0 2 2 3 3 3 1 0 0]]\n",
            "\n",
            "Episodios: 10000\n"
          ]
        }
      ],
      "source": [
        "policy_sarsa_4 = SARSA(w4, actions, S,  alpha=0.5, gamma=0.9, epsilon = 0.1, max_iterations=10000, contador_max=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0k74lEvemGcM",
        "outputId": "d8ce8ed1-2b72-41ec-f981-246281c74e24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mundo\n",
            "[ O  X  O  X  O  O  O  O  O  X  O  O  O  O  O  X  X  X  O  X  O \n",
            " O  X  O  X  X  X  X  X  O  X  X  X  X  X  O  O  O  X  O  X  O \n",
            " O  X  O  O  O  O  O  O  O  X  O  O  O  X  O  X  X  X  O  X  O \n",
            " O  X  O  X  O  X  O  X  O  X  O  X  O  O  O  O  X  O  O  X  O \n",
            " O  O  O  X  O  X  O  X  X  X  X  X  X  X  X  O  X  O  X  X  O \n",
            " X  X  X  X  O  X  O  O  O  X  O  O  O  O  O  O  X  O  O  O  O \n",
            " O  O  O  O  O  X  X  X  O  X  X  X  X  X  X  O  X  X  O  X  O \n",
            " X  X  X  X  O  X  O  X  O  X  O  O  O  O  O  O  O  O  O  X  O \n",
            " O  O  O  X  O  O  O  X  X  X  O  O  X  X  X  X  X  X  X  X  O \n",
            " O  X  O  X  O  X  O  X  O  O  O  X  X  O  O  O  O  O  O  X  X \n",
            " O  X  O  X  O  X  X  X  O  X  O  X  O  O  X  X  X  X  O  O  O \n",
            " O  X  O  X  O  X  O  O  O  X  O  X  O  X  X  O  O  X  X  X  O \n",
            " O  X  O  O  O  X  X  O  X  X  O  X  O  X  O  O  O  O  O  X  O \n",
            " O  X  X  X  X  X  O  O  X  O  O  O  O  O  O  X  X  X  O  X  O \n",
            " O  O  O  O  X  O  O  X  X  O  X  O  X  X  O  X  O  O  O  X  O \n",
            " X  X  X  O  O  O  X  X  O  O  X  O  O  X  X  X  O  X  X  X  X \n",
            " O  O  X  X  O  X  X  X  X  X  X  X  O  O  O  X  O  X  O  O  O \n",
            " X  O  O  X  O  X  O  O  O  X  O  O  O  X  X  X  O  X  O  X  O \n",
            " X  X  O  O  O  X  X  X  O  X  X  X  O  O  O  X  O  O  O  X  O \n",
            " O  X  X  O  X  X  O  O  O  O  O  X  O  X  X  X  X  X  X  X  O \n",
            " O  O  O  O  O  O  O  X  X  X  O  X  O  O  O  O  O  O  O  X  F ]\n",
            "\n",
            "Política\n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m < \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m < \u001b[0m v \u001b[0m < \u001b[0m v \u001b[0m < \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m < \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m < \u001b[0m ^ \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m ^ \u001b[0m X \u001b[0m ^ \n",
            "\u001b[0m ^ \u001b[0m < \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m ^ \n",
            "\u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m < \u001b[0m v \u001b[0m X \u001b[0m > \u001b[0m < \u001b[0m < \u001b[0m < \u001b[0m < \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m v \u001b[0m v \u001b[0m ^ \n",
            "\u001b[0m < \u001b[0m < \u001b[0m < \u001b[0m < \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \n",
            "\u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m v \u001b[0m < \u001b[0m < \u001b[0m < \u001b[0m ^ \u001b[0m < \u001b[0m < \u001b[0m ^ \u001b[0m X \u001b[0m ^ \n",
            "\u001b[0m v \u001b[0m < \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m v \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m < \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m v \u001b[0m < \u001b[0m < \u001b[0m < \u001b[0m < \u001b[0m v \u001b[0m X \u001b[0m X \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m < \u001b[0m v \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m > \u001b[0m v \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m v \u001b[0m v \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m < \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m ^ \u001b[0m ^ \u001b[0m < \u001b[0m v \u001b[0m X \u001b[0m ^ \n",
            "\u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m v \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m ^ \u001b[0m v \u001b[0m ^ \u001b[0m < \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \n",
            "\u001b[0m ^ \u001b[0m < \u001b[0m < \u001b[0m v \u001b[0m X \u001b[0m v \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m < \u001b[0m ^ \u001b[0m X \u001b[0m ^ \n",
            "\u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m v \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m > \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m v \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \n",
            "\u001b[0m < \u001b[0m v \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m < \u001b[0m < \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m < \u001b[0m v \n",
            "\u001b[0m X \u001b[0m ^ \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m v \u001b[0m < \u001b[0m v \u001b[0m X \u001b[0m > \u001b[0m < \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \n",
            "\u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m v \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m < \u001b[0m < \u001b[0m X \u001b[0m ^ \u001b[0m < \u001b[0m ^ \u001b[0m X \u001b[0m ^ \n",
            "\u001b[0m v \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m v \u001b[0m < \u001b[0m ^ \u001b[0m < \u001b[0m v \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \n",
            "\u001b[0m ^ \u001b[0m v \u001b[0m v \u001b[0m ^ \u001b[0m v \u001b[0m v \u001b[0m ^ \u001b[0m X \u001b[0m X \u001b[0m X \u001b[0m ^ \u001b[0m X \u001b[0m ^ \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m v \u001b[0m X \u001b[0m F \n",
            "\n",
            "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 1 0 0 0 0]\n",
            " [0 0 0 2 1 2 1 2 0 0 1 2 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 1 0 1 0 0 0]\n",
            " [0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 2 1 0 3 2 2 2 2 0 0 0 1 1 0]\n",
            " [2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 0 0 0 1 1 2 2 2 0 2 2 0 0 0]\n",
            " [1 2 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 2 0 0 0 1 2 2 2 2 1 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 2 1]\n",
            " [0 0 0 0 0 0 3 1 0 0 0 0 0 0 0 1 1 0 0 0 0]\n",
            " [0 0 0 2 0 0 0 0 0 0 0 0 0 0 1 0 0 2 1 0 0]\n",
            " [0 0 0 0 0 0 1 0 0 1 0 1 0 2 0 0 0 0 0 0 0]\n",
            " [0 2 2 1 0 1 0 0 0 0 0 0 0 0 0 0 1 2 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 3 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [2 1 0 0 0 0 0 0 0 0 0 0 0 2 2 0 0 0 1 2 1]\n",
            " [0 0 1 0 0 0 1 2 1 0 3 2 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0 0 0 0 2 2 0 0 2 0 0 0]\n",
            " [1 0 0 0 0 0 1 2 0 2 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 1 1 0 1 1 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0]]\n",
            "\n",
            "Episodios: 50000\n"
          ]
        }
      ],
      "source": [
        "policy_q_learning_4 = Q_Learning(w4, actions, S, alpha=0.5, gamma=0.9, epsilon = 0.1, max_iterations=50000, contador_max=5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Análisis Mundo 4:** \n",
        "\n",
        "El mundo 4 tiene una complejidad superior al resto, y en este caso ningún algoritmo consigue resolverlo."
      ],
      "metadata": {
        "id": "xlWvGphIGROS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusión:**\n",
        "\n",
        "En general obsevamos que Q-learning proporciona soluciones mejores frente a SARSA, si bien a menudo requiere de un número mayor de iteraciones. Los mundos con mayor cantidad de obstáculos, como el mundo 1 y el 4, suelen presertar mayores dificutades, ya que las trayectorias para llegar al final tienen más interrupciones y los caminos son más largos. Así mismo, los mundos más abiertos, se benefician de un $\\epsilon$ más alto para explorar y encontrar la solución óptima."
      ],
      "metadata": {
        "id": "wlKGINDE3sJB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "riuTvtRW4yzI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}